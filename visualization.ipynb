{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7006dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_R_CLASS_SUBLIST = [\n",
    "        1, 2, 4, 6, 8, 9, 11, 13, 22, 23, 26, 29, 31, 39, 47, 63, 71, 76, 79, 84, 90, 94, 96, 97, 99, 100, 105, 107,\n",
    "        113, 122,\n",
    "        125, 130, 132, 144, 145, 147, 148, 150, 151, 155, 160, 161, 162, 163, 171, 172, 178, 187, 195, 199, 203,\n",
    "        207, 208, 219,\n",
    "        231, 232, 234, 235, 242, 245, 247, 250, 251, 254, 259, 260, 263, 265, 267, 269, 276, 277, 281, 288, 289,\n",
    "        291, 292, 293,\n",
    "        296, 299, 301, 308, 309, 310, 311, 314, 315, 319, 323, 327, 330, 334, 335, 337, 338, 340, 341, 344, 347,\n",
    "        353, 355, 361,\n",
    "        362, 365, 366, 367, 368, 372, 388, 390, 393, 397, 401, 407, 413, 414, 425, 428, 430, 435, 437, 441, 447,\n",
    "        448, 457, 462,\n",
    "        463, 469, 470, 471, 472, 476, 483, 487, 515, 546, 555, 558, 570, 579, 583, 587, 593, 594, 596, 609, 613,\n",
    "        617, 621, 629,\n",
    "        637, 657, 658, 701, 717, 724, 763, 768, 774, 776, 779, 780, 787, 805, 812, 815, 820, 824, 833, 847, 852,\n",
    "        866, 875, 883,\n",
    "        889, 895, 907, 928, 931, 932, 933, 934, 936, 937, 943, 945, 947, 948, 949, 951, 953, 954, 957, 963, 965,\n",
    "        967, 980, 981,\n",
    "        983, 988]\n",
    "IMAGENET_R_CLASS_SUBLIST_MASK = [(i in IMAGENET_R_CLASS_SUBLIST) for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef081b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy \n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms.v2 as v2 \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "import datasets\n",
    "\n",
    "from methods.tact_utils import get_PCs, remove_PCs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(cam):\n",
    "    result = []\n",
    "    for img in cam:\n",
    "        img = img - np.min(img)\n",
    "        img = img / (np.max(img)-np.min(img))\n",
    "        result.append(img)\n",
    "    result = np.float32(result)\n",
    "    plt.imshow(np.transpose(result, (1, 2, 0))) \n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad5604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rgb_image(image):\n",
    "    result = []\n",
    "    for img in image:\n",
    "        img = img - np.min(img)\n",
    "        img = img / (np.max(img)-np.min(img))\n",
    "        result.append(img)\n",
    "    result = np.float32(result)\n",
    "    result = np.transpose(result, (1, 2, 0))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175409c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_WEIGHT_DICT = {\n",
    "    'vit_b_32': models.ViT_B_32_Weights.IMAGENET1K_V1,\n",
    "}\n",
    "\n",
    "base_augmentation = v2.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "non_causal_augmentation = v2.Compose([\n",
    "                v2.ToDtype(torch.uint8, scale=True),\n",
    "                v2.RandAugment(),\n",
    "                v2.ToDtype(torch.float32, scale=True),\n",
    "                v2.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988847df",
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "    'dataset': 'imagenet_r',\n",
    "    'data_dir': '/path/to/data/dir',\n",
    "    'eval_batch_size': 8,\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**arguments)\n",
    "device = torch.device('cpu')\n",
    "\n",
    "dataset_class = getattr(datasets, args.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d259d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vit_b_32(weights=PRETRAINED_WEIGHT_DICT['vit_b_32']).to(device)\n",
    "test_loader = dataset_class.getTestLoader(args, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf24756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(model, x):\n",
    "    return model.heads(x)\n",
    "        \n",
    "def featurize(model, x):  \n",
    "    x = model._process_input(x)\n",
    "    n = x.shape[0]\n",
    "    batch_class_token = model.class_token.expand(n, -1, -1)\n",
    "    x = torch.cat([batch_class_token, x], dim=1)\n",
    "\n",
    "    x = model.encoder(x)\n",
    "    x = x[:, 0]\n",
    "    return x\n",
    "\n",
    "classifier_weight = model.heads[0].weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b5fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AttributeModel(nn.Module):\n",
    "    def __init__(self, model, weight):\n",
    "        super().__init__()\n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.weight = copy.deepcopy(weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model._process_input(x)\n",
    "        n = x.shape[0]\n",
    "\n",
    "        batch_class_token = self.model.class_token.expand(n, -1, -1)\n",
    "        x = torch.cat([batch_class_token, x], dim=1)\n",
    "\n",
    "        x = self.model.encoder(x)\n",
    "        x = x[:, 0]\n",
    "        if len(self.weight.size()) == 2:\n",
    "            return torch.mm(x, self.weight.T)\n",
    "        else:\n",
    "            y_hat = torch.bmm(x.unsqueeze(1), self.weight.transpose(1, 2))\n",
    "            y_hat = y_hat.squeeze(1)\n",
    "            return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399947d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def scale_cam_image(cam, target_size=None):\n",
    "    result = []\n",
    "    for img in cam:\n",
    "        img = img - np.min(img)\n",
    "        img = img / (np.max(img)-np.min(img))\n",
    "        if target_size is not None:\n",
    "            img = cv2.resize(img, target_size)\n",
    "        result.append(img)\n",
    "    result = np.float32(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "def visualise_CAM(image, cam):\n",
    "    cam = scale_cam_image(cam, target_size=(224,224))\n",
    "    image = scale_cam_image(image.numpy())\n",
    "    image = np.transpose(image, (1,2,0))\n",
    "    visualization = show_cam_on_image(image, cam, use_rgb=True)\n",
    "\n",
    "    CAM = np.uint8(255*cam)\n",
    "    CAM = cv2.merge([CAM, CAM, CAM])\n",
    "    images = np.hstack((np.uint8(255*image), CAM , visualization))\n",
    "    return images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dabb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_transform(tensor, height=7, width=7):\n",
    "    result = tensor[:, 1 :  , :].reshape(tensor.size(0),\n",
    "        height, width, tensor.size(2))\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8091d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_aug = 256\n",
    "remove = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9da5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_num, (x, y) in enumerate(test_loader):\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    base_x = [base_augmentation(current_x) for current_x in x]\n",
    "    base_x = torch.stack(base_x).to(device)\n",
    "    features = featurize(model, base_x)\n",
    "    features_under_augmentation = [features.detach()]\n",
    "    for _ in range(max_aug):\n",
    "        augmented_x = non_causal_augmentation(x)\n",
    "        feature_under_aug = featurize(model, augmented_x) \n",
    "        features_under_augmentation.append(feature_under_aug.detach())\n",
    "    features_under_augmentation = torch.stack(features_under_augmentation)\n",
    "    \n",
    "    current_features = features_under_augmentation[:max_aug+1]\n",
    "    current_features = current_features.transpose(0, 1)\n",
    "    _, V, mean = get_PCs(current_features)\n",
    "    \n",
    "    update_f = remove_PCs(features, mean, V, remove)\n",
    "    \n",
    "    model_weight = classifier_weight\n",
    "    model_weight = torch.stack([model_weight for _ in range(features.size(0))])\n",
    "    projected_prototype = remove_PCs(model_weight, mean, V, remove)\n",
    "    \n",
    "    old_pred = classifier(model, features)[:, IMAGENET_R_CLASS_SUBLIST_MASK].argmax(1)\n",
    "    new_pred = classifier(model, update_f)[:, IMAGENET_R_CLASS_SUBLIST_MASK].argmax(1)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(torchvision.utils.make_grid(base_x.reshape(-1, 3, 224, 224),4, 2).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc1a5c7",
   "metadata": {},
   "source": [
    "GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8f0266",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_y = [IMAGENET_R_CLASS_SUBLIST[y[i]] for i in range(y.size(0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57da8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [ClassifierOutputTarget(i) for i in project_y]\n",
    "attr_model = AttributeModel(model, classifier_weight)\n",
    "\n",
    "# https://github.com/jacobgil/pytorch-grad-cam/blob/master/tutorials/vision_transformers.md\n",
    "gradcam = GradCAM(model=attr_model, target_layers=[attr_model.model.encoder.layers[-1].ln_1], reshape_transform=reshape_transform)\n",
    "grayscale_cam = gradcam(input_tensor=base_x, targets=targets, aug_smooth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b788d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tact_attr_model = AttributeModel(model, projected_prototype)\n",
    "\n",
    "tact_gradcam = GradCAM(model=tact_attr_model, target_layers=[tact_attr_model.model.encoder.layers[-1].ln_1], reshape_transform=reshape_transform)\n",
    "tact_grayscale_cam = tact_gradcam(input_tensor=base_x, targets=targets, aug_smooth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b2380",
   "metadata": {},
   "outputs": [],
   "source": [
    "cams_on_image = [\n",
    "    show_cam_on_image(get_rgb_image(base_x[i].numpy()), grayscale_cam[i], use_rgb=True, image_weight=0.5) for i in range(base_x.size(0))\n",
    "]\n",
    "cams_on_image = np.stack(cams_on_image).transpose(0, 3, 1, 2)\n",
    "\n",
    "show_image(torchvision.utils.make_grid(torch.as_tensor(cams_on_image), 4, 2).numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b997e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "tact_cams_on_image = [\n",
    "    show_cam_on_image(get_rgb_image(base_x[i].numpy()), tact_grayscale_cam[i], use_rgb=True, image_weight=0.5) for i in range(base_x.size(0))\n",
    "]\n",
    "tact_cams_on_image = np.stack(tact_cams_on_image).transpose(0, 3, 1, 2)\n",
    "\n",
    "show_image(torchvision.utils.make_grid(torch.as_tensor(tact_cams_on_image), 4, 2).numpy())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip_torch2",
   "language": "python",
   "name": "clip_torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
